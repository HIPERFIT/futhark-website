---
title: Running Futhark on the NVIDIA Jetson Nano
author: Troels Henriksen
description: How to run Futhark code, and the Futhark compiler, on NVIDIAs small SoC.
---

I recently got my hands on an `NVIDIA Jetson Nano
<https://developer.nvidia.com/embedded/jetson-nano-developer-kit>`_,
which NVIDIA describes as "a small, powerful computer that lets you
run multiple neural networks in parallel".  In practice, it's a lot
like a souped-up Raspberry Pi, with a quad-core ARM CPU, a 128-core
Maxwell-based NVIDIA GPU, and 4GiB of RAM, and a power consumption of
5W. Quite slow compared to a real computer, but fast enough that you
can do interesting things with it.  Some people are using them for
`self-driving cars
<https://medium.com/@feicheung2016/getting-started-with-jetson-nano-and-autonomous-donkey-car-d4f25bbd1c83>`_
or `automated doorbells
<https://medium.com/@ageitgey/build-a-hardware-based-face-recognition-system-for-150-with-the-nvidia-jetson-nano-and-python-a25cb8c891fd>`_,
but I'll probably just make it render pretty fractals on the wall
display in my office.  Since I long ago exceeded my tolerance for
writing GPU code by hand, the first step is of course to figure out a
way to run Futhark on the device.  While the Jetson does not support
OpenCL, The Futhark compiler `now has a CUDA backend
<https://futhark-lang.org/blog/2019-02-08-futhark-0.9.1-released.html#the-cuda-backend>`_,
so it should be possible.  This blog post documents how to get it
working.

I'll be assuming that you have a freshly installed Jetson Nano with a
working CUDA setup, meaning that you can run ``nvcc`` in the command
line and compile CUDA programs.  For inexplicable reasons, NVIDIA does
not set the environment variables correctly out of the box, but
setting the following should take care of it::

  export PATH=${PATH}:/usr/local/cuda/bin
  export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/cuda/lib64

You will need a root partition with at least 32GiB of space.

There are two ways of running Futhark code on the Jetson:

* Run ``futhark cuda`` on some other machine, copy the generated
  ``.c`` file to the Jetson, and then compile to a binary there.
  Since the C code generated by the Futhark compiler is not machine
  specific, it can easily be moved.

* Run an ARM build of the Futhark compiler on the Jetson itself.

I'll cover the former option first, since it is much simpler.  When
you run ``futhark cuda foo.fut``, the Futhark compiler will generate a
file ``foo.c`` and a binary ``foo``.  You can then move that ``foo.c``
to the Jetson and compile it with::

  $ gcc foo.c -o foo -O -std=c99 -lm -lcuda -lnvrtc

Note that if your host system does not itself support CUDA,
compilation of ``foo`` will fail.  However, ``foo.c`` remains
generated, so you can still copy it to the Jetson and finish
compilation there.  It's not pretty, but it works.  If you use
``futhark cuda --library``, which you `likely will for real use
<https://futhark.readthedocs.io/en/latest/usage.html#compiling-to-library>`_,
then ``gcc`` is not invoked for you, so you will not see any error.

Compiling the Futhark compiler on the Jetson
--------------------------------------------

The Jetson uses an ARM CPU, and Futhark binary releases are currently
only available for x86-64.  Hence, we'll have to recompile the Futhark
compiler from scratch.  This is `normally a straightforward procedure
<https://futhark.readthedocs.io/en/latest/installation.html#compiling-from-source>`_,
but a little more tricky when using an exotic architecture (ARM) and a
small machine (the Jetson).  Specifically, The Futhark compiler is
written in `Haskell <https://www.haskell.org/>`_, and while the
`Glasgow Haskell Compiler <https://www.haskell.org/ghc/>`_ (GHC) does
support ARM, it is not a so-called "tier 1 platform", meaning that
binary releases are spotty.  This `looks like it will change in the
future
<https://www.reddit.com/r/haskell/comments/c1rk8w/whats_the_story_with_ghc_on_arm/erg3ryi/>`_,
but for now, it takes some effort to get a usable Haskell
infrastructure set up on the Jetson.

Ideally, we'd cross-compile an ARM build of Futhark from a beefier
machine, but cross-compiling is notoriously difficult, and I could not
get it to work.  Instead, we'll be compiling Futhark on the Jetson
itself.  Futhark uses the `Stack build tool
<https://haskellstack.org>`_, which fortunately comes compiled for
ARM::

  $ curl -sSL https://get.haskellstack.org/ | sh

Unfortunately, Futhark's Stack configuration specifies GHC 8.6.5, and
the newest official binary release of GHC on ARM is 8.4.2.  While in
theory we could use GHC 8.4.2 to compile GHC 8.6.5 on the Jetson, this
would take an extremely long time.  Instead, we will be `using the Nix
package manager <https://nixos.org/nix/download.html>`_, which has
binary releases of recent GHCs.  Installing Nix is non-invasive (we
will not be using all of `NixOS <https://nixos.org/>`_, which is
definitely invasive)::

  $ curl https://nixos.org/nix/install | sh

While this saves us from compiling GHC itself, we still have to
compile a *lot* of Haskell, and GHC always hungers for memory.
However, GHC also hungers for RAM disk space (specifically
``/var/run``), and the default cap of 10% of physical memory is not
sufficient.  Edit ``/etc/systemd/logind.conf`` and set
``RuntimeDirectorySize=30%``.  Reboot after this.  If you have more
systemd knowledge than I, maybe you can avoid the reboot.

Memory-wise, the 4GiB in the Jetson is also not enough.  Therefore,
set up a 4GiB swap file::

  # sudo fallocate -l 4G /swapfile
  # sudo chmod 600 /swapfile
  # sudo mkswap /swapfile
  # sudo swapon /swapfile

This setup is transient, meaning it'll go away on next reboot, but
you'll have to delete ``/swapfile`` yourself.

Now clone the `Futhark Git repository
<https://github.com/diku-dk/futhark>`_ as usual and run::

  $ stack --nix install --fast -j1

The ``--nix`` part tells ``stack`` to fetch GHC from Nix, rather than
use a non-existent official release.  ``--fast`` disables the Haskell
optimiser, which saves on time and space.  ``-j1`` limits concurrency
to one job, also to limit memory usage.  You may be able to bump this
higher (say, ``-j4``) to speed up compilation.  If the build crashes
at some point due to an out-of-memory situation, simply reduce it to
``-j1`` and carry on.  All dependencies that managed to be
succesfully built should still be available.

The build need not finish in one sitting, which is good, because this
will take a *long* time.  When it's done, you'll have a ``futhark``
binary located in ``$HOME/.local/bin``.  To verify that it works, try
running part of the Futhark test suite::

  $ futhark test --backend=cuda examples

Hopefully, it should work.  Congratulations!  You can now compile and
run Futhark programs on the Jetson.  There are no other
Jetson-specific considerations that I have detected.  Unfortunately,
the CUDA backend is for C, not Python, although it may be possible
that we create a `PyCUDA <https://documen.tician.de/pycuda/>`_ backend
someday.  If you want to easily show some graphics, consider `Lys
<https://github.com/diku-dk/lys>`_, which will certainly also be the
topic of a coming blog post.
